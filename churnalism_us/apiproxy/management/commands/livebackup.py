"""
This command is the complement of the restorebackup command. It 
iterates over documents on the server, pickles them, then dumps
them into a file and finally zips that file up with a small data
containing the number of documents, etc.
"""

import sys
import os
import itertools
from copy import deepcopy
from tempfile import NamedTemporaryFile
from zipfile import ZipFile, ZIP_DEFLATED
from contextlib import closing
try:
    import cPickle as pickle
except ImportError:
    import pickle
from django.core.management.base import BaseCommand
import superfastmatch


class PushBackIterator(object):
    def __init__(self, subiter):
        self.subiter = iter(subiter)

    def __iter__(self):
        return self

    def next(self):
        return self.subiter.next()

    def pushback(self, obj):
        self.subiter = itertools.chain([obj], self.subiter)


class ChunkedIterator(object):
    """
    >>> it = ChunkedIterator(range(0, 6), chunksize=2, key=lambda obj: 1)
    >>> [list(chunk) for chunk in it]
    [[0, 1], [2, 3], [4, 5]]
    >>> it = ChunkedIterator(range(0, 5), chunksize=3, key=lambda obj: 1)
    >>> [list(chunk) for chunk in it]
    [[0, 1, 2], [3, 4]]
    >>> it = ChunkedIterator(range(0, 7), chunksize=3, key=lambda obj: obj)
    >>> [list(chunk) for chunk in it]
    [[0, 1, 2], [3], [4], [5], [6]]
    >>> it = ChunkedIterator([], chunksize=2, key=lambda obj: 1)
    >>> [list(chunk) for chunk in it]
    []
    """
    def __init__(self, subiter, chunksize, key):
        self.subiter = PushBackIterator(iter(subiter))
        self.chunksize = chunksize
        self.key = key

    def __iter__(self):
        return self

    def next(self):
        chunk = []
        current_size = 0
        try:
            while current_size < self.chunksize:
                obj = self.subiter.next()
                obj_size = self.key(obj)
                new_size = current_size + obj_size
                if new_size > self.chunksize and len(chunk) > 0:
                    self.subiter.pushback(obj)
                    return chunk
                chunk.append(obj)
                current_size = new_size
        except StopIteration:
            if len(chunk) == 0:
                raise

        return chunk

def prune_document(docmeta):
    """Removes the attributes that will be regenerated by the restore process.
    """
    ignored_attributes = ['characters', 'id']

    doc = deepcopy(docmeta)
    for attr in ignored_attributes:
        if attr in doc:
            del doc[attr]
    return doc


class Command(BaseCommand):
    help = 'Dumps a set of documents from SuperFastMatch into a pickle file.'
    args = '<file> [doctypes]'

    def handle(self, outpath, doctype_rangestr, *args, **options):
        sfm = superfastmatch.DjangoClient()
        
        if not doctype_rangestr:
            print "It's usually not a good idea to dump all documents."
        else:
            if os.path.exists(outpath):
                print "I have nothing against {0}, why would I overwrite it?".format(outpath)
                return

            doctypes = superfastmatch.parse_doctype_range(doctype_rangestr)

            docs = superfastmatch.DocumentIterator(sfm, 
                                                   order_by='docid', 
                                                   doctype=doctype_rangestr,
                                                   chunksize=1000,
                                                   fetch_text=True)

            chunked_docs = ChunkedIterator(docs,
                                           chunksize=10000000, # approx. 10 megabytes
                                           key=lambda d: len(d.get('text')))

            metadata = {
                'doctypes': doctypes,
                'doc_count': 0,
                'file_count': 0
            }
           
            with closing(ZipFile(outpath, 'w', compression=ZIP_DEFLATED, allowZip64=True)) as outfile:
                with NamedTemporaryFile(mode='wb') as metafile:
                    for (file_number, docs_chunk) in enumerate(chunked_docs):
                        if len(docs_chunk) == 0:
                            continue

                        with NamedTemporaryFile(mode='wb') as docsfile:
                            for docmeta in docs_chunk:
                                if not docmeta:
                                    print >>sys.stderr, "Dropped empty document."
                                    continue
                                try:
                                    doc = prune_document(docmeta)
                                    pickle.dump(doc, docsfile, pickle.HIGHEST_PROTOCOL)
                                    metadata['doc_count'] += 1
                                except Exception as e:
                                    print >>sys.stderr, str(e)

                            docsfile.flush()
                            print "Compressing backup chunk #{num} containing {chunksize} documents...".format(num=file_number, chunksize=len(docs_chunk))
                            outfile.write(docsfile.name, 'docs{num}'.format(num=file_number))
                            metadata['file_count'] += 1
                    print "Dumped {doc_count} documents spanning doctypes {doctypes}".format(**metadata)
                    pickle.dump(metadata, metafile)
                    metafile.flush()
                    outfile.write(metafile.name, 'meta')
        print "Done."
            

if __name__ == "__main__":
    import doctest
    doctest.testmod()

